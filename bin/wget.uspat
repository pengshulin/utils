#!/usr/bin/env python
# batch download US patent pdfs from www.pat2pdf.org
# designed by Peng Shulin <trees_peng@163.com>

import urllib, urllib2, cookielib
import BeautifulSoup
import os, sys

PAT_FETCH_URL = 'http://www.pat2pdf.org/pat2pdf/foo.pl'
PAT_FETCH_RESULT_PREFIX = 'http://www.pat2pdf.org/patents/pat'
RETRY_TIMES = 5

cj = cookielib.LWPCookieJar()
opener = urllib2.build_opener(urllib2.HTTPCookieProcessor(cj))
opener.addheaders = [
    ("User-agent", "Mozilla/5.0 (Windows; U; Windows NT 5.1; zh-CN; rv:1.9.0.11) Gecko/2009060215 Firefox/3.0.11"),
    ("Accept", "text/html, image/jpeg, image/png, text/*, image/*, */*"),
    ("Accept-Language", "zh-cn,zh;q=0.5") ]

    
def download_pat( patid ):
    print 'Patent %s downloading...'%patid
    if os.path.isfile( '%s.pdf'%patid ):
        print '%s.pdf exists, ignore'%patid
        return True
    postcontents = { 'number' : '%s'%patid }
    r = opener.open( PAT_FETCH_URL, urllib.urlencode(postcontents) )
    soup = BeautifulSoup.BeautifulSoup( ''.join(r.readlines()) )
    #print soup
    s = soup.find('a',attrs={'href':'/patents/pat%s.pdf'%patid})
    if s:
        # patent pdf found
        os.system( 'wget %s%s.pdf -O %s.pdf'%(PAT_FETCH_RESULT_PREFIX,patid,patid) )
        return True
    else:
        print 'Failed'
        return False


def retry_down( patstr ):
    patid = patstr.strip().lower().replace(',','')
    if not patid:  return
    t = RETRY_TIMES
    while t:
        if download_pat( patid ):  break
        t -= 1

def main(argv=None):
    if argv is None:
        argv = sys.argv
    if len(sys.argv) != 2:
        print 'Usage: wget.uspat patent_id / patent_id_list_file'
        return 1

    arg = sys.argv[1]
    if not os.path.isfile( arg ):
        # patent id assigned
        retry_down( arg )
    else:
        # patent_list_file assigned
        for i in open( arg, 'r' ).readlines():
            retry_down( i.split()[0] )
    

if __name__ == '__main__':
    sys.exit(main())


